{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape generation with a Reinforcement Learning agent\n",
    "\n",
    "This notebook is an experiment with shape generation using fonts as datasets. (0-9 a-z A-Z)<p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "I'm looking at ways to use AI to create design tools, where the AI would serve as a digital assistant during the design process. One problem is that computers and humans view design in very different ways, bridging that gap is one central part of AI research. Here I'm trying out an idea i had regarding this<p>\n",
    "    \n",
    "We have seen Reinforcement models excel in deterministic environments, such as go, chess or atari games. What if you were to design a game that teach the agent a skill that is useful outside of the game environment?<p>\n",
    "    \n",
    "![ ](imgs/3d_letter.PNG)\n",
    "\n",
    "In this game a blindfolded agent is dropped down on a letter, such as the letter \"t\" above, allthough the agent doesn't know which letter. The agent then explores the unknown world, pixel by pixel, getting rewarded for picking a safe pixel.\n",
    "\n",
    "By training our agent we will end up with a model that inputs the current state (the environment known by the agent), and outputs a probability map for the next move.<p>\n",
    "    \n",
    "What would happen if the trained agent is dropped down on a big open plane? <b>Will it still move in the shape of letters?</b>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's load our dataset. It's been created by a dataset generator available in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_image( infilename ):\n",
    "    \n",
    "    img = Image.open( infilename )\n",
    "    img.load()\n",
    "    data = np.asarray( img, dtype=\"int32\" )\n",
    "    return data\n",
    "\n",
    "path = os.getcwd()+'/arial_/'\n",
    "\n",
    "flist=[]\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    try:\n",
    "        flist.append(load_image(path+\"/\"+filename))\n",
    "    except:\n",
    "        None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the images into training data, and show one random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 62\n",
      "random_i: 51\n",
      "pixels: 57\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa6940c6160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_all=np.array(flist)\n",
    "x_all=np.delete(x_all,[1,2],axis=3)\n",
    "\n",
    "def set_example(i):\n",
    "    ex=x_all[i]/255*-1+1\n",
    "    ex=(ex>0.3)*1\n",
    "    return ex\n",
    "\n",
    "num_examples=len(flist)\n",
    "print(\"Number of examples: \"+str(num_examples))\n",
    "\n",
    "fi=np.random.randint(num_examples)\n",
    "\n",
    "x_inp=set_example(fi)\n",
    "fi+=1\n",
    "print(\"random_i: \"+str(fi))\n",
    "print(\"pixels: \"+str(np.count_nonzero(x_inp)))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(40, 8))\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "plt.imshow(x_inp.reshape(100,100), interpolation=\"nearest\")\n",
    "plt.gray()\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a outline function\n",
    "since we will limit our agent to explore pixels adjacent to pixels allready explored. By running our input state through the outline function, we have a boolean mask of our possible next moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  1.]\n",
      " [ 1.  0.  0.  1.]\n",
      " [ 0.  1.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "def outline(s_in):\n",
    "    s=np.pad(s_in,1,'constant')\n",
    "    s_0=np.roll(s,-1,axis=0)\n",
    "    s_1=np.roll(s,-1,axis=1)\n",
    "    s10=np.roll(s,1,axis=0)\n",
    "    s11=np.roll(s,1,axis=1)\n",
    "    \n",
    "    o=s_0+s_1+s10+s11\n",
    "    o=(o[1:-1,1:-1]>0)*(-s_in+1)\n",
    "    return o\n",
    "    \n",
    "t=np.zeros((4,4))\n",
    "t[2,1]=1\n",
    "t[2,2]=1\n",
    "t[1,2]=1\n",
    "print(outline(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Neural networks.\n",
    "\n",
    "The state is beeing fed through two different networks. The softmax layer is used to pick the next move. However, it'll only outputs the probabilities of the different actions <b>relative to each other</b><br>\n",
    "\n",
    "We need another network to output the overall prediction, how certain is the agent that the next move will be successful?<p>\n",
    "    \n",
    "These are simple deep learning models, should probably try out some regularization methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Reshape\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def valueNetwork(x):\n",
    "    inp_size=ws*ws\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    p = Dense(inp_size, activation='softmax')(x)  \n",
    "    return p\n",
    "\n",
    "def predict_model(x):\n",
    "    inp_size=ws*ws\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    #x = Dense(32, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    #x = Dense(32, activation='relu')(x)\n",
    "    x = Dense(8, activation='relu')(x)\n",
    "    p = Dense(1, activation='sigmoid')(x)  \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some more initialization\n",
    "Most notable is the window size \"ws\" which is the agents scope of view. This sets the size of the state that is fed through the networks. This makes it possible for the agent to create letters that are bigger than the state\n",
    "\n",
    "(insert picture of canvas and state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ws=25             \n",
    "wl=np.floor(ws/2).astype(int)\n",
    "wu=np.ceil(ws/2).astype(int)\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "state = tf.placeholder(tf.float32, shape=(None,ws*ws))\n",
    "Y = tf.placeholder(tf.float32, shape=(None,ws*ws))\n",
    "\n",
    "model=valueNetwork(state)\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(model), reduction_indices=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0001)\n",
    "update = optimizer.minimize(loss)\n",
    "\n",
    "PY = tf.placeholder(tf.float32, shape=(None,1))\n",
    "pmodel=predict_model(state)\n",
    "ploss = tf.nn.sigmoid_cross_entropy_with_logits(labels=PY,logits=pmodel)\n",
    "poptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0001)\n",
    "pupdate = poptimizer.minimize(ploss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/johan_lagerloef/geniter/tf_models/arial.ckpt\n",
      "iteration: 50 loss: 2052.04361456\n",
      "iteration: 100 loss: 1794.60437455\n",
      "iteration: 150 loss: 1992.7219102\n",
      "iteration: 200 loss: 1683.94467849\n",
      "iteration: 250 loss: 1835.47098807\n",
      "iteration: 300 loss: 1527.83338101\n",
      "iteration: 350 loss: 2073.40827018\n",
      "iteration: 400 loss: 1782.85047218\n",
      "iteration: 450 loss: 1745.71468574\n",
      "iteration: 500 loss: 1596.09654932\n",
      "iteration: 550 loss: 1505.39625941\n",
      "iteration: 600 loss: 1720.33574913\n",
      "iteration: 650 loss: 1849.36338972\n",
      "iteration: 700 loss: 1589.22237814\n",
      "iteration: 750 loss: 1658.71298264\n",
      "iteration: 800 loss: 2042.1786025\n",
      "iteration: 850 loss: 1687.88641794\n",
      "iteration: 900 loss: 1680.87690328\n",
      "iteration: 950 loss: 1659.96314542\n",
      "iteration: 1000 loss: 1713.53357665\n",
      "Model saved in path: /home/johan_lagerloef/geniter/tf_models/arial.ckpt\n",
      "iteration: 1050 loss: 1574.3894044\n",
      "iteration: 1100 loss: 1878.78862239\n",
      "iteration: 1150 loss: 1437.00201805\n",
      "iteration: 1200 loss: 1623.479009\n",
      "iteration: 1250 loss: 1588.6546445\n",
      "iteration: 1300 loss: 1616.3745131\n",
      "iteration: 1350 loss: 1676.66722046\n",
      "iteration: 1400 loss: 1505.70308053\n",
      "iteration: 1450 loss: 1882.24553842\n",
      "iteration: 1500 loss: 1546.53547485\n",
      "iteration: 1550 loss: 1708.62066525\n",
      "iteration: 1600 loss: 1663.53397316\n",
      "iteration: 1650 loss: 1482.0756163\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "#Comment next line to start training from scratch\n",
    "saver.restore(sess, os.getcwd()+\"/tf_models/arial.ckpt\")\n",
    "loss_acc=0.0\n",
    "\n",
    "for i in range(10000): #Total number of training examples during training\n",
    "    fi=np.random.randint(num_examples)\n",
    "    x_inp=set_example(fi)\n",
    "    \n",
    "    \n",
    "    #Initializing the startpoint by picking the top left pixel\n",
    "    max0=np.max(x_inp,axis=0)\n",
    "    ty=np.argmax(max0)\n",
    "    tx=np.argmax(x_inp[:,ty])\n",
    "    \n",
    "    #Alternative method with random initializing\n",
    "    #startpoint=[np.random.randint(48),np.random.randint(48)] \n",
    "    #while x_inp[startpoint[0],startpoint[1]]==0:\n",
    "    #    startpoint=[np.random.randint(48),np.random.randint(48)]\n",
    "    #tx,ty=startpoint[0],startpoint[1]\n",
    "\n",
    "    #Initialize canvas\n",
    "    canvas=np.zeros((200,200))\n",
    "    px=100\n",
    "    py=100\n",
    "    canvas[px,py]=1\n",
    "\n",
    "    a_check=0.0\n",
    "    hits=0\n",
    "    miss=0\n",
    "    streak=0\n",
    "    streaks=[]\n",
    "    states=[]\n",
    "    rewards=[]\n",
    "    \n",
    "    wrong=np.zeros(ws*ws)\n",
    "    \n",
    "    ex_iter=48 #Based on how many pixels there is in the training examples.\n",
    "    for j in range(ex_iter):\n",
    "\n",
    "        s=canvas[px-wl:px+wu,py-wl:py+wu].reshape(1,ws*ws) #get the current state\n",
    "        p=sess.run(model, {state:s}).reshape(ws*ws) #run the state through valueModel\n",
    "        p_masked=(p.reshape(ws,ws)*outline(s.reshape(ws,ws))).reshape(ws*ws) #remove actions not adjacent to the state\n",
    "\n",
    "        p_masked=p_masked*(-wrong+1) #remove actions known to be wrong\n",
    "        p_norm=p_masked/np.sum(p_masked) #the normalized distribution\n",
    "\n",
    "        action = np.random.choice(ws*ws,p=p_norm) #pick random move from the distribution\n",
    "\n",
    "        action_oh = np.eye(ws*ws)[action] # create one hot vector\n",
    "\n",
    "\n",
    "        test=x_inp[tx-wl:tx+wu,ty-wl:ty+wu].reshape(ws*ws) #compare with x_inp (the truth)\n",
    "\n",
    "        reward=test*action_oh\n",
    "        wrong+=((-test+1)>0)*(action_oh>0)\n",
    "        wrong=(wrong>0)*1\n",
    "\n",
    "        \n",
    "        if np.max(reward)==1:\n",
    "            \n",
    "            #update the predict network\n",
    "            sess.run([pupdate,ploss] , {state:s, PY:np.ones((1,1))})\n",
    "            \n",
    "            hits+=1\n",
    "            streak+=1\n",
    "            s=s.reshape(ws*ws)\n",
    "            \n",
    "            #add sucsessful moves to a list, the success streak. \n",
    "            states.append(s)\n",
    "            rewards.append(reward.reshape(ws*ws))\n",
    "            \n",
    "            #update the valueNetwork with THE ENTIRE STREAK. To promote successful policys\n",
    "            _,lossout=sess.run([update, loss], {state:np.array(states), Y:np.array(rewards)})\n",
    "  \n",
    "            ux=action//ws-wl\n",
    "            uy=action%ws-wl  \n",
    "            px+=ux\n",
    "            py+=uy\n",
    "            tx+=ux\n",
    "            ty+=uy\n",
    "            canvas[px,py]=1\n",
    "            wrong=np.zeros(ws*ws)\n",
    "            \n",
    "        if np.max(reward)==0:\n",
    "            #_,lossout = \n",
    "            sess.run([pupdate,ploss] , {state:s, PY:np.zeros((1,1))})\n",
    "            \n",
    "            miss+=1\n",
    "            streaks.append(streak)\n",
    "            streak=0\n",
    "            states=[]\n",
    "            rewards=[]\n",
    "            \n",
    "        loss_acc+=lossout\n",
    "\n",
    "    streaks.append(streak)\n",
    "    if (i+1) % 50 == 0:\n",
    "        print(\"iteration: \"+str(i+1)+\" loss: \"+str(loss_acc))\n",
    "        loss_acc=0\n",
    "    if (i+1) % 1000 == 0:\n",
    "        save_path = saver.save(sess, os.getcwd()+\"/tf_models/arial.ckpt\")\n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possibility to restore saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/johan_lagerloef/geniter/tf_models/arial.ckpt\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, os.getcwd()+\"/tf_models/arial.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let it roam free!\n",
    "At this point the agent is trained and ready to be tested out in a free environment. Run the cell over and over the see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random actions: 5Iterations: 65\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS0AAAEtCAYAAABd4zbuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAABMZJREFUeJzt3DFOG1EUQNE3EQU9G2A3LCLrS/bAbrwB+nQ/BUVQFBsTbMPF51SWZ2T96up9z+hva60BqPj20QsAeAvRAlJEC0gRLSBFtIAU0QJSRAtIuTnmpm3b7mbmYWZ2M/PrnAsCrtbtzNzPzONa62nfTUdFa56D9eMEiwJ4zfeZ+bnv4rHbw91JlgLwut2hi8dGy5YQuJSDvfFHPJAiWkCKaAEpogWkiBaQIlpAimgBKaIFpIgWkCJaQIpoASmiBaSIFpAiWkCKaAEpogWkiBaQIlpAimgBKaIFpIgWkCJaQIpoASmiBaSIFpAiWkCKaAEpogWkiBaQIlpAimgBKaIFpIgWkCJaQIpoASmiBaSIFpAiWkCKaAEpogWkiBaQIlpAimgBKaIFpIgWkCJaQIpoASmiBaSIFpAiWkCKaAEpogWkiBaQIlpAimgBKaIFpIgWkCJaQIpoASmiBaSIFpAiWkCKaAEpogWkiBaQIlpAimgBKaIFpIgWkCJaQIpoASmiBaSIFpAiWkCKaAEpogWkiBaQIlpAimgBKaIFpIgWkCJaQIpoASmiBaSIFpAiWkCKaAEpogWkiBaQIlpAimgBKaIFpIgWkCJaQIpoASmiBaSIFpAiWkCKaAEpogWkiBaQIlpAimgBKaIFpIgWkCJaQIpoASmiBaSIFpAiWkCKaAEpogWkiBaQIlpAimgBKaIFpIgWkCJaQIpoASmiBaSIFpAiWkCKaAEpogWkiBaQIlpAimgBKaIFpIgWkCJaQIpoASmiBaSIFpAiWkCKaAEpogWkiBaQIlpAys1HL4Cva631z++3bbvwSvhKTFpAimgBKaIFpIgWkCJaQIpoASmiBaSIFpAiWkCKaAEpogWkiBaQIlpAimgBKaIFpDhPi4vbd87W/3I+13UxaQEpogWk2B7yabxlm/dyi/nys63i12fSAlJEC0ixPeTiTrGFe/kbp34ayedm0gJSRAtIES0gRbSAFNECUkQLSBEtIEW0gBQvl5LkhdLrZdICUkQLSBEtIEW0gBTRAlJEC0gRLSDFe1qcjfPaOQeTFpAiWkCKaAEpogWkiBaQ4ukhF3fqExo8pbwuJi0gRbSAFNtDkmwJr5dJC0gxaXFxpiTew6QFpIgWkCJaQIpoASmiBaSIFpAiWkCKaAEpogWkiBaQIlpAimgBKaIFpIgWkCJaQIpoASmiBaSIFpAiWkCKaAEpogWkiBaQIlpAimgBKaIFpIgWkCJaQIpoASmiBaSIFpAiWkCKaAEpogWkiBaQIlpAimgBKaIFpIgWkCJaQIpoASmiBaSIFpAiWkCKaAEpogWkiBaQIlpAimgBKaIFpIgWkCJaQIpoASmiBaSIFpAiWkCKaAEpogWkiBaQIlpAimgBKaIFpIgWkCJaQIpoASmiBaSIFpAiWkCKaAEpogWkiBaQIlpAimgBKaIFpIgWkCJaQIpoASmiBaSIFpAiWkCKaAEpogWkiBaQIlpAimgBKaIFpIgWkCJaQIpoASmiBaQcG63bs64C4I+DvTk2WvfvXwfAUe4PXdzWWq/+wrZtdzPzMDO7mfl1ilUB/OV2noP1uNZ62nfTUdEC+Cz8EQ+kiBaQIlpAimgBKaIFpIgWkCJaQMpvoCc0Wx7VvcgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3582359e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "canvas=np.zeros((100,100))\n",
    "px=py=50\n",
    "canvas[px,py]=1\n",
    "ra=0\n",
    "j=0\n",
    "while j<100:\n",
    "    j+=1\n",
    "    s=canvas[px-wl:px+wu,py-wl:py+wu].reshape(1,ws*ws)\n",
    "    p=sess.run(model, {state:s}).reshape(ws*ws)\n",
    "    p=(p.reshape(ws,ws)*outline(s.reshape(ws,ws))).reshape(ws*ws)\n",
    "    p=p/np.sum(p)\n",
    "    action = np.argmax(p) #pick the most likely action\n",
    "    \n",
    "    ps=sess.run(pmodel, {state:s}).reshape(1)\n",
    "    \n",
    "    if ps<0.7:  # If the predict model outputs a value below a threshold, we pick a random action instead. \n",
    "                # Without randomness the model would output the same shape every time.\n",
    "        ra+=1\n",
    "        action = np.random.choice(ws*ws,p=p)\n",
    "        if j>60:\n",
    "            break\n",
    "\n",
    "    ux=action//ws-wl\n",
    "    uy=action%ws-wl  \n",
    "    px+=ux\n",
    "    py+=uy\n",
    "    canvas[px,py]=1\n",
    "       \n",
    "print(\"Random actions: \"+str(ra)+\"Iterations: \"+str(j))\n",
    "\n",
    "plt.figure(figsize=(40, 8))\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "im=plt.imshow(canvas, interpolation=\"nearest\")\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do: Summary, evaluation and possible improvements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
